\setcounter{secnumdepth}{4}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}


In this chapter, first, we learn about the risks associated with banking. Then we will discuss Asset Liability Management (ALM) a solution to minimize risk associated with banking. Then we will discuss project objectives and problem statement. Then a glance over banking standards and the literature survey.\cite{}

\section{Risks Associated with Banking}

The term risk can be defined in association with banking as the exposure to loss.  The several risks associated with banking are viz.,
Operational risk, Credit risk, Liquidity risk, Market risk, Foreign exchange risk, Interest rate risk and Information risk.

	\subsection{Operational Risk}

		It is defined as the loss occurred due to failure of peoples or system. It is faced in the various departments as of Information Technology department, Credit Department, Investment department.

	\subsection{Credit Risk}

		It is defined as the loss occurred due to failure of the borrower to repay to bank on the agreed terms. There is uncertainty about the repayment of dues and in the repayment in the agreed time frame. It happens due to borrowers lack of income, failure of business or reluctance to repay and lack of underwriting frameworks.

	\subsection{Market risk}

		It is defined as the loss occurred due to fluctuation in the market prices. Its components are as follows:
		
		\begin{itemize}
		
			\item \textbf{Equity risk: }Probable failure to generate profit due to fluctuation in stock prices.
			
			\item \textbf{Foreign exchange risk: }Probable failure to generate profit due to the fluctuation of exchange rates as bank does transaction in multiple currencies.
			
			\item \textbf{Interest rate risk: }Probable failure to generate profit due to the fluctuation of interest rates.
			
			\item \textbf{Commodity risk: }Probable failure to generate profit due to change in commodity prices as metals (as gold, silver, platinum), Energy (oil and gas) and Agriculture (as wheat, cotton, coffee, tea,etc.). The change in these prices occurs due to variations in demand and supply.

		\end{itemize}

	\subsection{Liquidity risk}

		It is characterized as the lack of liquid cash available in hands of the bank to finance its day to day transactions or the situation where bank runs out of cash. Failure in managing the liquidity risk can lead to the destruction of the banks' reputation and losing its trust among customers.

The exposure of banks to these various risks and as the risks are also increasing with the liberalisation and increasing merger of local markets and global markets. Due to the deregulated operational space and the various products that requires the interest rates to be determined with the need to maintain the proper ratios between the profitability, spread and long term growth. Due to all these reasons, we need to perform Asset Liability management.


	
\section{Asset Liability Management (ALM)}

	An asset is a valuable entity/resource that a person/organization owns for generating income.  And the liability is a valuable entity/resource that a person/organization needs to pay for. At any given point of time, total assets must be greater than the total liability. To balance the equation the concept of asset liability management \cite{11} had emerged. 

	In ALM we perform periodic monitoring of risk exposures involving collecting and analyzing the information in order to have the ability to anticipate, forecast and act so as to structure banks business to profit. ALM also involves transforming the asset and liability portfolio in a dynamic way to manage the risks which involve judgment and decision making.

%ALM involves the planning, directing and controlling the cash flow and yield of the consolidated funds of the bank for all %assets and liabilities of a bank by rate, amount and maturity. It assesses the various asset mixes, funding combinations, %price volume relations and their implications on liquidity, income and capital ratio.


	
\section{ALM for Banks}

	As the business of banking involves lot of risks, the main problem of banking becomes risk management and the procedure to do so is ALM. The three pillars on which ALM resides are follows:

	\subsection{ALM Information System}
		The risk policies and tolerance limits need to be specified by ALM information system. Information is the key to ALM process which is now available due to the computerization of all banks and its respective branches. It also includes performing experimentation in a specific branch and studying its effects. If the results are positive then replicating the changes to other branches.

	\subsection{ALM Organization}
		For risk management to be successful the need is strong commitment of the senior management to take strategic decisions and integrate the basic operations. The Asset Liability Committee (ALCO) is formed for performing the above-mentioned tasks. It includes the CEO and the senior management of the bank, and their task is to decide business strategies with respect to banks budget and decide risk management objectives according to assets and liabilities. 

	\subsection{ALM Processes}
		The ALM Processes are as shown in the following figure:
		
		\begin{center}
		\includegraphics[width=\linewidth]{figures/ALM_Process.jpg}	
		\captionof{figure}{ALM Processes}
		\label{fig:ALM Processes}
		\end{center}
		

		All the risks discussed in section 1.1 are the problems to be handled by the ALM Processes. The assets and liabilities of a bank involve the following:

		\begin{center}
		\includegraphics[width=\linewidth]{figures/Components_of_Banks_Balance_Sheet.jpg}	
		\captionof{figure}{Components of Banks Balance Sheet}
		\label{fig:Components of Banks Balance Sheet}
		\end{center}
	
		The ALM cycle of a bank is as follows:

		\begin{center}
		\includegraphics[width=\linewidth]{figures/ALM_Cycle_of_a_Bank.jpg}	
		\captionof{figure}{ALM Cycle of a Bank}
		\label{fig:ALM Cycle of a Bank}
		\end{center}

		Throughout the cycle of the ALM the mismatch in asset and liabilities (negative gap) should not exceed 20\% of the cash outflow during 2-14 days and 15-28 days time bucket.

	
\section{Project Objective}

	To study the strategies to stabilize financial networks i.e. banks and to improve the profitability of them from various risks. 

\section{Problem Statement}

	The two problems that have been studied are as follows:
	\subsection{Liquidity Risk Management}

Simulation of the ALM concept for bank using the 1999 Czech Financial data set and try the single objective cash flow optimization for ALM to ensures Liquidity and Profitability of the bank. 

	\subsection{Prediction of Stock Price}

		The Prediction of Stock Price is a typical problem of time series and we use a deep learning based solution for future price prediction.

\section{Literature Survey}

	For first problem the literature survey done is: Hongxi Li et al. (2017) assess for both positive and negative duration gap to increase the net value of bank when interest rates fluctuate favorably\cite{4}. Nalan Gülpinar et al. (2013) uses the Vector Autoregressive process to model time-varying investment opportunities\cite{3}. Teng Fan et al. (2011) studied the interest rate risk of Chinese life insurers’ liability\cite{1}. Mounika, P et al. (2011) have addressed the problem of single objective optimization for the maximization of wealth\cite{16}. Chaudhury, Rahul et al. (2014) created a fuzzy rule-based asset liability optimization model\cite{17}.

	For the second problem, the literature survey done is: Ashish Vaswani et al. (2017) proposed an attention mechanism for machine translation\cite{2}. Yao Qin et al. (2017) proposed a DA-RNN. It serves the purpose of attention to time series and encoding information of long sequences\cite{6}. Jian Liu et al. (2017) assesses the correlation between stock price movement with relevance to events happening in the world\cite{7}. Hao Li et al. (2018) proposed the MI-LSTM using attention which filters noise and extracts information\cite{8}. Huicheng Liu (2018) used attention based RNN for leveraging the news to predict stock price\cite{9}.

Below are the existing algorithms which helps as basis for problems we are studying. The implementation of some of these methods is available in a software package called \emph{statsmodels, pandas, sklearn} \cite{5} \cite{12}, which are also necessary statistical analysis.


%--------------------------------------------------------------------------------------------------------------------------------------

\subsection{Time Series Analysis for Stock Price Prediction}

	\subsubsection{Introduction}
		The Time Series (TS) is defined as the collection of information or data at a regular interval of time. This TS contains the time-dependent data which may contain the seasonality (i.e the deviation in data at specific time frame) and trend (i.e. the changing mean with respect to time) in it; which is the change in data with respect to specific time frame.

		Ex. The Stock Data is collected per day i.e. the value of stock at the start of the day, at the end of the day, the highest value of the day and the lowest value of the day.

		\begin{center}
		\includegraphics[width=\linewidth]{figures/Ex_of_Time_Series_data_of_a_stock.jpg}	
		\captionof{figure}{Ex of Time Series data of a stock}
		\label{fig: Ex of Time Series data of a stock}
		\end{center}


	
	\subsubsection{Issues with non-Stationarity of Data}
	
		The data is said to be stationary if the mean and variance of the data is stable over time and it has time-independent autocovariance. The statistical models which deal with the TS data have a premise that the data should be stationary. Because of this premise, we need to convert non-stationary data to stationary data. But how to check the data is stationary or not? There are two ways to do it:
		\begin{itemize}
		
			\item Plotting the Rolling Statistics
			
			\item Dickey-Fuller Test

		\end{itemize}
		
		\paragraph{Plotting the Rolling Statistics}

			In this procedure, we visualize the moving average and/or moving variance by plotting it against time.

		\begin{center}
		\includegraphics[width=\linewidth]{figures/Moving-Average-and-Moving-Variance.jpg}	
		\captionof{figure}{ Moving Average and Moving Variance of the Closing Price with window size = 5}
		\label{fig: Moving Average and Moving Variance of the Closing Price with window size = 5}
		\end{center}

		\begin{center}
		\includegraphics[width=\linewidth]{figures/Plot-of-the-Rolling-Mean-and-the-Rolling-Standard-Deviation.jpg}	
		\captionof{figure}{Plot of the Rolling Mean and the Rolling Standard Deviation of Moving Average of Closing Price}
		\label{fig: Plot of the Rolling Mean and the Rolling Standard Deviation of Moving Average of Closing Price}
		\end{center}
	
			As we can infer from the plot, the rolling mean is changing with respect to time and there is a clear fluctuation in the standard deviation resulting in the data to be non-Stationary.

		\paragraph{Dickey-Fuller Test (DF Test)}

		In statistics, there is a unit root test that is used to check the given TS is non-Stationary or not. DF Test \cite{18} is the most widely used unit root test. In statistical testing we have to make the hypothesis, the same in this test are:

H0:	TS is non-Stationary.
H1:	TS is Stationary.

H0 is the Null hypothesis and the H1 is the Alternate Hypothesis. The hypothesis H0 is accepted if the p-value of the test is greater than 5\% else the H0 is rejected and H1 is accepted.

For the data we are dealing with the results of the DF Test are as follows:

		\begin{center}
		\includegraphics[width=\linewidth]{figures/DF-Test-Results-for-moving-average-of-Closing-price.jpg}	
		\captionof{figure}{DF Test Results for moving average of Closing price}
		\label{fig: DF Test Results for moving average of Closing price}
		\end{center}
	
		As the p-value is 0.1236 i.e. 12.36\% which is greater than the threshold 5\% so the null hypothesis H0 is accepted; meaning the TS is non-Stationary.

	\subsubsection{Converting non-Stationary TS to Stationary TS}
	
		To make the TS stationary we need to eliminate the trend and seasonality from the non-stationary TS. The first and the widely used approach to do it is Transforming the data using a log transform. Then use either Differencing or Decomposition. Let’s see them one by one:

		\paragraph{Log Transform}
			Taking the log of the data changes the recurrent pattern to a linear pattern and also stabilizes the variance of the data \cite{18}.

		\begin{center}
		\includegraphics[width=\linewidth]{figures/DF-Test-Results-for-log-transformed-Closing-price.jpg}	
		\captionof{figure}{DF Test Results for log-transformed Closing Price}
		\label{fig: DF Test Results for log-transformed Closing Price}
		\end{center}

			Here we can see that the p-value is dropped from 12.36\% to 9.15\% which means that we can say we have 90\% confidence that the TS is stationary.

		\paragraph{Decomposing}

			A TS consists of three components: trend, seasonality and residual. If the decomposition be additive then:
				\begin{equation}
					Data\textsubscript{t} = Trend\textsubscript{t} + Seasonality\textsubscript{t} + Residual\textsubscript{t}  
				\end{equation}
				


			Where the subscript t denotes the time t.
			The multiplicative decomposition is:
				\begin{equation}
					Data\textsubscript{t} = Trend\textsubscript{t} * Seasonality\textsubscript{t} * Residual\textsubscript{t} 
				\end{equation}
				

			The addition/multiplication of these three components gives back the original data or TS. The multiplicative decomposition is used when the trend/seasonality of data is proportional to the time.
			The procedure of classical multiplicative decomposition \cite{18} is as follows:

				\begin{algorithm}[H]
					\caption{Classical Multiplicative Decomposition}
					Assumption m - seasional period or frequency, 
	        						MA - moving average, 
	       						DT\textsubscript{t} - Detrended series.

					\begin{algorithmic}[1] 
						\STATE If m is even number then \begin{equation}Trend_{t} = 2 * m-MA		\end{equation}

								Else \begin{equation}Trend_{t} = m-MA	\end{equation}
						\STATE Compute Detrended series:
									\begin{equation}DT\textsubscript{t} = Data\textsubscript{t} - Trend\textsubscript{t} \end{equation}
						\STATE The seasonal component Seasonalitytof the respective season is the average of the DT\textsubscript{t} of the given season.
						\STATE \begin{equation}Residual\textsubscript{t} = Data\textsubscript{t} / (Trend\textsubscript{t}  * Seasonality\textsubscript{t})	\end{equation}
					\end{algorithmic}
				\end{algorithm}

			The plot looks as follows:
				
				\begin{center}
				\includegraphics[width=\linewidth]{figures/The-trend-seasonality-and-residual-of-log-transformed-data.jpg}	
				\captionof{figure}{The trend, seasonality and residual of log-transformed data}
				\label{fig: The trend, seasonality and residual of log-transformed data}
				\end{center}

			The result of DF test results on the residual data is as follows:

				\begin{center}
				\includegraphics[width=\linewidth]{figures/DF-Test-Results-on-residual-data.jpg}	
				\captionof{figure}{DF Test Results on residual data}
				\label{fig: DF Test Results on residual data}
				\end{center}

			Here the p-value is 7.5e-17\% which means that we can say the TS is stationary with 99.99\% confidence.

		\paragraph{Differencing}

			In this approach, we compute the difference between the data point at the current instance to the previous instance. Applying the DF test on the differencing \cite{18} of the log-transformed data gives the following results:

				\begin{center}
				\includegraphics[width=\linewidth]{figures/DF-Test-Results-for-log-transformed-Closing-price-after-differencing.jpg}	
				\captionof{figure}{DF Test Results for log-transformed Closing price after differencing}
				\label{fig: DF Test Results for log-transformed Closing price after differencing}
				\end{center}

			Here the p-value is 0.0\% which means that we can say the TS is stationary with 100\% confidence.


	\subsubsection{Disadvantages of Making TS stationary}

		There is huge information loss when we are converting the non-stationary TS to Stationary TS. The elimination of trend and seasonality leads to information loss which makes the model linear and it fails to predict the future trend and seasonality appropriately. 

	\subsubsection{Inference}

		The statistical methods are good to work with the stationary data, but when the data is non-stationary the statistical methods fail due to the premise discussed in section 3.2. Although due to the advances in the research area of neural networks we are able to deal with the non-stationary data without converting it to stationary. The methods used to predict the TS data are discussed in following section.


\subsection{Deep Learning Architectures for Prediction of Stock Price}

In this chapter, we will study the different deep learning techniques used to predict the sequence of output after feeding them the time-based input sequence. 

For all the tasks consider
Data: {X\textsubscript{i} : Source\textsubscript{i} ; Y\textsubscript{i} : Target\textsubscript{i} }
And the loss function: Mean Squared Error Loss as the problem is a regression problem. 

\subsubsection{Recurrent Neural Network (RNN)}

Artificial neural networks have a special class of neural networks that works best with the input sequences, called the RNN. As stated in the name the word Recurrent stands for the repeating structure of the neurons with respect to time/sequence based input to them until the whole input sequence is over. Unlike other neural network architectures where we need to feed whole input at once to the input neurons; in RNN \cite{12}, we can feed the sequence one by one to the input neurons and the input gets processed in the same recurrent fashion throughout all the recurrent layers. The output of the neuron after processing the (t-1)$^{th}$ input sequence is fed again to the same neuron with the (t)$^{th}$ sequence of the input so that it can remember the whole input sequence. The idea basically is to remember the past, add it to the present and predict the future. The RNN looks as shown in the following Fig.

				\begin{center}
				\includegraphics[width=\linewidth]{figures/An-unrolled-recurrent-neural-network.jpg}	
				\captionof{figure}{Unfolded Recurrent layer}
				\label{fig: Unfolded Recurrent layer}
				\end{center}


Now let’s consider the Fig.4.3 which shows the activation function tanh in the neurons of recurrent layers. The equation of the recurrent layer is as follows:

\begin{equation}
	h_{t} = tanh (w_{i} * X_{t} + w_{o} * h_{t-1} + b)
\end{equation}


Where wi is the weight associated with the inputs and the wo is the weights associated with the output of the previous state. Using these weights we will learn about the sequenced input.

				\begin{center}
				\includegraphics[width=\linewidth]{figures/The-repeating-module-in-a-standard-RNN.jpg}	
				\captionof{figure}{Unfolded Recurrent layer with activation function}
				\label{fig: Unfolded Recurrent layer with activation function}
				\end{center}


\paragraph{Problems of Long Term Dependency}

If the input sequences are small then the RNN is the best choice, but what if the input sequence is long, can RNN successfully learn the information from the past and using present information predict the future? The answer is No. In RNN we don’t have an equation for how much to remember from the past or what to remember from the past. This is called the problem of long term dependency \cite{12} as shown in Fig.4.4. If the output ht+1 depends on the input X1 then RNN can’t handle this dependency. The solution to this is the improved version of RNN discussed later.

				\begin{center}
				\includegraphics[width=\linewidth]{figures/Problem-of-Long-Term-Dependency.jpg}	
				\captionof{figure}{Long term dependency problem}
				\label{fig: Long term dependency problem}
				\end{center}



\paragraph{Long Short Term Memory (LSTM)}

The word LSTM \cite{12} can be interpreted as storing the Short information in the Memory for the Long Term. This happens due to the functional change in the cell of RNN. Instead of the single neuron function in the RNN, the LSTM has four neurons connected in a circuit creating three gates: input gate, forget gate and the output gate. Other than these gates there is a cell state which computes the update for the next state. The following are the notations that will be used to describe the architecture of the LSTM:

				\begin{center}
				\includegraphics[width=\linewidth]{figures/Notations.jpg}	
				\captionof{figure}{Notations}
				\label{fig: Notations}
				\end{center}

The comparison of RNN and LSTM is as follows:

				\begin{center}
				\includegraphics[width=\linewidth]{figures/Comparison-of-RNN-and-LSTM-Cells.jpg}	
				\captionof{figure}{Comparison of RNN and LSTM State}
				\label{fig: Comparison of RNN and LSTM State}
				\end{center}

\subparagraph{Cell State}

The cell state decides how much of the original information from previous state is to be retained to the next state (Forget gate) and how much new information from the current state is to be added after forgetting the old information from the previous state (Input gate). It's like scaling and shifting operation. The information from the previous state is scaled between 0 to 1 and information from the current state is added (shifting).

				\begin{center}
				\includegraphics[width=\linewidth]{figures/Cell-State.jpg}	
				\captionof{figure}{Cell State}
				\label{fig: Cell State}
				\end{center}

\subparagraph{Forget Gate}

As discussed in the cell state this gate uses the sigmoid function which gives the values between 0 to 1. These values determine the scaling of the forget gate.
Equation for forget gate:

\begin{equation}
	f_{t} = \sigma (W_{f} . [h_{t-1}, X_{t}] + b_{f})	
\end{equation}

				\begin{center}
				\includegraphics[width=\linewidth]{figures/Forget-gate.jpg}	
				\captionof{figure}{Forget Gate}
				\label{fig: Forget Gate}
				\end{center}

\subparagraph{Input Gate}

This gate decides how much information from the current state is to be added to the cell state. It uses a sigmoid and a tanh function for this purpose. The equation is as follows:

\begin{equation}
	i_{t} = \sigma (W_{i} . [h_{t-1}, X_{t}] + b_{i})
\end{equation}

\begin{equation}
	\tilde{I}_{t} = tanh (W_{c} . [h_{t-1}, X_{t}] + b_{c})	
\end{equation}


The sigmoid function ranges between 0 to 1 and tanh function ranges between -1 to 1 which shifts the input accurately and creates the output of the input gate.

				\begin{center}
				\includegraphics[width=\linewidth]{figures/Input-gate.jpg}	
				\captionof{figure}{Input Gate}
				\label{fig: Input Gate}
				\end{center}

\subparagraph{Updating Cell State}

The eq(2.8 to 2.10) computed the values for updating the cell state, now we just need to do pointwise multiplication and addition. The equation is as follows:

\begin{equation}
		C_{t} = f_{t} * C_{t-1} + i_{t} * \tilde{I}_{t}
\end{equation}

				\begin{center}
				\includegraphics[width=\linewidth]{figures/update-to-new-cell-state.jpg}	
				\captionof{figure}{Update to new cell state}
				\label{fig: Update to new cell state}
				\end{center}


\subparagraph{Output Gate}

Using sigmoid function on current states input and tanh function on the updated cell state we can output the parts we need. The equations are as follows:

\begin{equation}
	o_{t} = \sigma (W_{o} . [h_{t-1}, X_{t}] + b_{o})
\end{equation}

\begin{equation}
	h_{t}  = o_{t} * tanh( C_{t} )
\end{equation}

				\begin{center}
				\includegraphics[width=\linewidth]{figures/output-gate.jpg}	
				\captionof{figure}{Output Gate}
				\label{fig: Output Gate}
				\end{center}

The Fig. below shows the unfolded LSTM layer.

For the understanding of the later equations let us consider the following equations:
For RNN: \begin{equation} h_{t} = RNN(h_{t-1}, X_{t}) \end{equation}

For LSTM: \begin{equation} h_{t}, C_{t} = LSTM(h_{t-1}, C_{t-1}, X_{t}) \end{equation} 

\paragraph{Encoder - Decoder Models (Sequence to sequence models)}

The Encoder - Decoder model \cite{12} is very much useful in a lot of applications. The Encoder which is a neural network works for computing the representation of the input while the decoder which is also a neural network uses this input representation from Encoder and the target output to learn about the relation between the input and output and makes the appropriate predictions. For our problem, both encoder and decoder use the RNN/LSTM as the encoding and decoding function. The Encoder - Decoder mechanism can be represented as in Fig. below:

				\begin{center}
				\includegraphics[width=\linewidth]{figures/Sequence-to-Sequence-Model.jpg}	
				\captionof{figure}{Encoder - Decoder Mechanism}
				\label{fig: Encoder - Decoder Mechanism}
				\end{center}

The equations for the encoder-decoder model using RNN as the neural network architecture is as follows:
 
Encoder: \begin{equation} h_{t} = RNN(h_{t-1}, X_{t}) \end{equation}

Decoder: \begin{equation} C_{0} = h_{t}  \end{equation}	
		 \begin{equation} C_{t} = RNN(C_{t-1}, [h_{t-1}, e(\hat{y}_{t-1})])  \end{equation}

%Ct = RNN(Ct-1, [ht-1, e(y_hatt-1)])

\paragraph{Encoder - Decoder Models with attention (Sequence to sequence models with attention)}

The attention is a technique which tells the decoder how much to focus on the information at the given point of time. The attention of the overall input sequence adds to 1 due to the softmax function. The Encoder - Decoder mechanism with attention \cite{2}, \cite{7} can be represented as in Fig.4.13.

				\begin{center}
				\includegraphics[width=\linewidth]{figures/Attention-Model.jpg}	
				\captionof{figure}{Encoder - Decoder Mechanism with Attention}
				\label{fig: Encoder - Decoder Mechanism with Attention}
				\end{center}

The equations for the encoder-decoder model with attention using RNN as the neural network architecture is as follows:

Encoder: \begin{equation} h_{t} = RNN(h_{t-1}, X_{t}) \end{equation}
		\begin{equation} C_{0} = h_{t}  \end{equation}

Decoder: \begin{equation} e_{jt} = V_{attn} tanh(U * h_{j} + W * C_{t}) \end{equation}	
		 \begin{equation} \alpha_{jt} = softmax(e_{jt})  \end{equation}
		 \begin{equation} S_{t} = \sum_{j=1}^t\alpha_{it} * h_{j}  \end{equation}  
		 \begin{equation} C_{t} = RNN(C_{t-1}, [e(\hat{y}_{t-1}), S_{t}])  \end{equation} 

Where e$_{jt}$ is the importance of j$^{th}$ input for decoding the t$^{th}$ output and $\alpha$$_{jt}$ is the focus probability on j$^{th}$ input with respect to t$^{th}$ output.
	


\section{Overview of Project Report}

Write it at the end.